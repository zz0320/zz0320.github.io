<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CCourier+New:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/silver/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"username.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文将详细介绍神经网络中常见的五种归一化「包含Weight Norm」">
<meta property="og:type" content="article">
<meta property="og:title" content="「深度学习基础」神经网络中的归一化">
<meta property="og:url" content="https://username.github.io/2022/05/23/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8D%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82/index.html">
<meta property="og:site_name" content="陨石kk的博客">
<meta property="og:description" content="本文将详细介绍神经网络中常见的五种归一化「包含Weight Norm」">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jc4owol4j20no0wr0xv.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jev7jxj1j20nz0yjwj0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jn4yqfhyj20dc0dmdgv.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jn5dinr0j20fe0gcmyj.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jnl8z6l6j20lz09qjsb.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhis5uy6j20oc0ywjwi.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhpehlcqj20nn0u7gp0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jmddiyq1j20nw0psgo5.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jmpgy3afj20ty0j70uh.jpg">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190329214342134.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhplp99pj20r60e5gnk.jpg">
<meta property="article:published_time" content="2022-05-23T07:25:27.000Z">
<meta property="article:modified_time" content="2022-05-28T08:35:15.633Z">
<meta property="article:author" content="陨石kk">
<meta property="article:tag" content="深度学习基础">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jc4owol4j20no0wr0xv.jpg">


<link rel="canonical" href="https://username.github.io/2022/05/23/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8D%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://username.github.io/2022/05/23/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8D%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82/","path":"2022/05/23/「深度学习基础」归一化层/","title":"「深度学习基础」神经网络中的归一化"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>「深度学习基础」神经网络中的归一化 | 陨石kk的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">陨石kk的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization%E3%80%8C%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%8D"><span class="nav-number">1.</span> <span class="nav-text">Batch Normalization「批归一化」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Layer-Normalization%E3%80%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%8D"><span class="nav-number">2.</span> <span class="nav-text">Layer Normalization「层归一化」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Instance-Normalization%E3%80%8C%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%8D"><span class="nav-number">3.</span> <span class="nav-text">Instance Normalization「实例归一化」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Group-Normalization%E3%80%8C%E7%BE%A4%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%8D"><span class="nav-number">4.</span> <span class="nav-text">Group Normalization「群归一化」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Normalization%E3%80%8C%E6%9D%83%E9%87%8D%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%8D"><span class="nav-number">5.</span> <span class="nav-text">Weight Normalization「权重归一化」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE"><span class="nav-number">6.1.</span> <span class="nav-text">思维导图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE"><span class="nav-number">6.2.</span> <span class="nav-text">可视化图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E9%87%8F%E5%9B%BE%E8%A1%A8"><span class="nav-number">6.3.</span> <span class="nav-text">统计量图表</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="陨石kk"
      src="/uploads/%E5%A4%B4%E5%83%8F.jpg">
  <p class="site-author-name" itemprop="name">陨石kk</p>
  <div class="site-description" itemprop="description">无限进步</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zz0320" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zz0320" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zyc160378629@gmail.com" title="E-Mail → mailto:zyc160378629@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://username.github.io/2022/05/23/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8D%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/%E5%A4%B4%E5%83%8F.jpg">
      <meta itemprop="name" content="陨石kk">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="陨石kk的博客">
      <meta itemprop="description" content="无限进步">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="「深度学习基础」神经网络中的归一化 | 陨石kk的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          「深度学习基础」神经网络中的归一化
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-23 15:25:27" itemprop="dateCreated datePublished" datetime="2022-05-23T15:25:27+08:00">2022-05-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-05-28 16:35:15" itemprop="dateModified" datetime="2022-05-28T16:35:15+08:00">2022-05-28</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文将详细介绍神经网络中常见的五种归一化「包含Weight Norm」</p>
<iframe src="//player.bilibili.com/player.html?aid=595391170&bvid=BV1Pq4y1a7pH&cid=569272749&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<span id="more"></span>

<h2 id="Batch-Normalization「批归一化」"><a href="#Batch-Normalization「批归一化」" class="headerlink" title="Batch Normalization「批归一化」"></a>Batch Normalization「<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d">批归一化</a>」</h2><iframe src="//player.bilibili.com/player.html?aid=973300661&bvid=BV1X44y1r77r&cid=346242830&page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>通道级别归一化，贯穿整个mini-batch，per channel across mini-batch，即在<strong>整个Batch的所有样本，在单一维度上进行归一化</strong>。</p>
<p>论文最初是想通过它用来减少内部协变量转移，但后续有论文指出它可能就是通过<strong>在每个小批量上加入噪音来控制模型复杂度</strong>，<strong>让optimization landscape更平滑</strong>，因此没必要跟丢弃法混合使用</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.142ex" xmlns="http://www.w3.org/2000/svg" width="20.404ex" height="5.572ex" role="img" focusable="false" viewBox="0 -1516 9018.4 2462.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2080.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3136.2,0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mfrac" transform="translate(3679.2,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1121.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2121.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mo" transform="translate(329.3,-4) translate(-278 0)"><path data-c="2C6" d="M279 669Q273 669 142 610T9 551L0 569Q-8 585 -8 587Q-8 588 -7 588L12 598Q30 608 66 628T136 666L277 744L564 587L555 569Q549 556 547 554T544 552Q539 555 410 612T279 669Z"></path></g></g></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g><g data-mml-node="msub" transform="translate(1296.7,-789)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(285.5,-15) translate(-278 0)"><path data-c="2C6" d="M279 669Q273 669 142 610T9 551L0 569Q-8 585 -8 587Q-8 588 -7 588L12 598Q30 608 66 628T136 666L277 744L564 587L555 569Q549 556 547 554T544 552Q539 555 410 612T279 669Z"></path></g></g></g><g data-mml-node="mi" transform="translate(604,-150) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g><rect width="3310.7" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7452.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(8452.4,0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container><br>批量归一化具体操作是：固定小批量中的均值和方差，然后学习出适合的偏移和缩放</p>
<p>结果上来讲，可以<strong>加速收敛速度</strong>，但一般不改变模型精度</p>
<p>下面是Pytorch官方给出的解读</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jc4owol4j20no0wr0xv.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代码样例</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">time_steps = <span class="number">3</span></span><br><span class="line">embedding_dim = <span class="number">4</span></span><br><span class="line">inputx = torch.randn(batch_size, time_steps, embedding_dim) <span class="comment"># N * L * C </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 考虑三维输入（N*C*L）在整个Batch所有样本的固定Channel维度上进行Norm</span></span><br><span class="line"><span class="comment"># 实现batch_norm并验证API</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 batch_norm API</span></span><br><span class="line">batch_norm_op = torch.nn.BatchNorm1d(embedding_dim, affine=<span class="literal">False</span>) <span class="comment"># 传入C</span></span><br><span class="line">bn_y = batch_norm_op(inputx.transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#手写batch_norm</span></span><br><span class="line">bn_mean = inputx.mean(dim=(<span class="number">0</span>,<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">bn_std = inputx.std(dim=(<span class="number">0</span>,<span class="number">1</span>), unbiased=<span class="literal">False</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">verify_bn_y = (inputx - bn_mean) / (bn_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(bn_y)</span><br><span class="line"><span class="built_in">print</span>(verify_bn_y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#考虑四维输入 在Channel维度上进行norm</span></span><br><span class="line"><span class="comment"># With Learnable Parameters</span></span><br><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># Without Learnable Parameters</span></span><br><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>, affine=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>) <span class="comment"># B/N，C，H，W</span></span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Layer-Normalization「层归一化」"><a href="#Layer-Normalization「层归一化」" class="headerlink" title="Layer Normalization「层归一化」"></a>Layer Normalization「<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html?highlight=layernorm#torch.nn.LayerNorm">层归一化</a>」</h2><p>per sample，per layer「C*H*W」 即对批量内的<strong>单一样本</strong>进行计算，不考虑mini-batch，考虑每一层「NLP中的每个时间点」，用于解决BN无法很好地处理文本数据长度不一的问题</p>
<p>LN在CV上是在一张图片维度上进行计算，而在NLP上不同sample也即不同mini-batch的大小是不一样的，即CV中不同图片尺寸大小不一样</p>
<p>下面是Pytorch官方给的解读</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jev7jxj1j20nz0yjwj0.jpg"></p>
<p><strong>这里值得注意的是，在CV中，LN是指对一整张图片进行标准化处理，即在一张图片所有channel的pixel范围内计算均值和方差。而在NLP的问题中，LN是指在一个句子的一个token的范围内进行标准化。</strong></p>
<table>
<thead>
<tr>
<th>图像数据中的LN计算范围</th>
<th>NLP任务中的计算范围</th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jn4yqfhyj20dc0dmdgv.jpg"></td>
<td><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jn5dinr0j20fe0gcmyj.jpg"></td>
</tr>
</tbody></table>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jnl8z6l6j20lz09qjsb.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现layer_norm并验证API</span></span><br><span class="line">layer_norm_op = torch.nn.LayerNorm(embedding_dim, elementwise_affine=<span class="literal">False</span>)</span><br><span class="line">ln_y = layer_norm_op(inputx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写layer_norm </span></span><br><span class="line"><span class="comment"># H * C * L</span></span><br><span class="line">ln_mean = inputx.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">ln_std = inputx.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_ln_y = (inputx - ln_mean) / (ln_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(ln_y)</span><br><span class="line"><span class="built_in">print</span>(verify_ln_y)</span><br><span class="line"><span class="comment"># 之所以和CV不一样「CV传入C.H.W 而NLP只传入C」 是因为每个batch中的各个样本 L的长度不一致 防止padding后污染原先的norm</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43827595/article/details/121877901">参考链接</a>🔗</p>
</blockquote>
<h2 id="Instance-Normalization「实例归一化」"><a href="#Instance-Normalization「实例归一化」" class="headerlink" title="Instance Normalization「实例归一化」"></a>Instance Normalization「<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html?highlight=instance#torch.nn.InstanceNorm2d">实例归一化</a>」</h2><p>Instance Normalization常用于风格迁移， per sample per channel，对于每个样本的每个维度进行计算</p>
<blockquote>
<p>至于为什么可以用于风格迁移，可以理解为，归一化会将不变的东西消掉，也就是图片中的风格消掉</p>
</blockquote>
<p>下面是Pytorch官方给的解读</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhis5uy6j20oc0ywjwi.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现instance_norm并验证API</span></span><br><span class="line"><span class="comment"># 调用ins_norm并验证API</span></span><br><span class="line">ins_norm_op = torch.nn.InstanceNorm1d(embedding_dim)</span><br><span class="line">in_y = ins_norm_op(inputx.transpose(-<span class="number">1</span>, -<span class="number">2</span>)).tanspose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写ins_norm</span></span><br><span class="line"><span class="comment"># H * C * L</span></span><br><span class="line">in_mean = inputx.mean(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">in_std = inputx.std(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">verify_in_y = (inputx - in_mean) / (in_std + <span class="number">1e-5</span>)</span><br><span class="line"><span class="built_in">print</span>(in_y)</span><br><span class="line"><span class="built_in">print</span>(verify_in_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CV B*C*H*W</span></span><br><span class="line"><span class="comment"># Without Learnable Parameters</span></span><br><span class="line">m = nn.InstanceNorm2d(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># With Learnable Parameters</span></span><br><span class="line">m = nn.InstanceNorm2d(<span class="number">100</span>, affine=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Group-Normalization「群归一化」"><a href="#Group-Normalization「群归一化」" class="headerlink" title="Group Normalization「群归一化」"></a>Group Normalization「<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html?highlight=groupnorm#torch.nn.GroupNorm">群归一化</a>」</h2><p>Group Normalization，是per sample, per group，有点类似于Layer Norm，对input tensor划成不同的Group</p>
<p>下面是Pytorch官方给的解读</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhpehlcqj20nn0u7gp0.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现group_norm并验证API</span></span><br><span class="line"><span class="comment"># 调用group_norm并验证API</span></span><br><span class="line">num_groups = <span class="number">2</span></span><br><span class="line">group_norm_op = torch.nn.GroupNorm(num_groups, embedding)</span><br><span class="line">gn_y = group_norm_op(inputx.transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写group_norm</span></span><br><span class="line">group_inputxs = torch.split(inputx, split_size_or_sections=embedding_dim // num_groups, dim=-<span class="number">1</span>) <span class="comment"># 分组</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> g_inputx <span class="keyword">in</span> group_inputxs:</span><br><span class="line">  gn_mean = g_inputx.mean(dim=(<span class="number">1</span>, <span class="number">2</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">  gn_std = g_inputx.std(dim=(<span class="number">1</span>, <span class="number">2</span>), keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">  gn_result = (g_inputx-gn_mean) / (gn_std + <span class="number">1e-5</span>)</span><br><span class="line">  results.append(gn_result)</span><br><span class="line">verify_gn_y = torch.cat(results, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(gn_y)</span><br><span class="line"><span class="built_in">print</span>(verify_gn_y)</span><br></pre></td></tr></table></figure>



<h2 id="Weight-Normalization「权重归一化」"><a href="#Weight-Normalization「权重归一化」" class="headerlink" title="Weight Normalization「权重归一化」"></a>Weight Normalization「<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html?highlight=weight%20norm#torch.nn.utils.weight_norm">权重归一化</a>」</h2><p>权重归一化本质上是在做decompose weight into <strong>magnitude</strong> and <strong>direction</strong>，简单来说 就是保留了之前的<strong>方向</strong>，得到了一个新的<strong>幅度</strong></p>
<p>具体来讲，对一个参数做如下变换，得到新的权重</p>
<ul>
<li>「先除以模，再乘以一个新的幅度」</li>
</ul>
<p>下面是Pytorch官方给的解读</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jmddiyq1j20nw0psgo5.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现weight_norm并验证API</span></span><br><span class="line"><span class="comment"># 调用weight_norm API</span></span><br><span class="line">linear = nn.Linear(embedding_dim, <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">wn_linear = torch.nn.utils.weight_norm(linear)</span><br><span class="line">wn_linear_output = wn_linear(inputx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写weight_norm</span></span><br><span class="line">weight_direction = linear.weight/linear.weight.norm(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># 权重的方向</span></span><br><span class="line">weight_magnitude = wn_linear.weight_g  <span class="comment"># 权重的幅度</span></span><br><span class="line">weight_magnitude = torch.tensor([linear.weight[i,:].norm() <span class="keyword">for</span> i <span class="keyword">in</span> torch.<span class="built_in">range</span>(linear.weight.shape[<span class="number">0</span>])], dtype = torch.float32).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(weight_direciton.shape, weight_magnitude.shape)</span><br><span class="line">verify_wn_linear_output = inputx @ (weight_direction.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * (weight_magnitude.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(wn_linear_output)</span><br><span class="line"><span class="built_in">print</span>(verify_wn_linear_output)</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了神经网络中的五种常用归一化层，分别为BN、LN、IN、GN和WN</p>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jmpgy3afj20ty0j70uh.jpg"></p>
<h3 id="可视化图"><a href="#可视化图" class="headerlink" title="可视化图"></a>可视化图</h3><p><img src="https://img-blog.csdnimg.cn/20190329214342134.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70"></p>
<h3 id="统计量图表"><a href="#统计量图表" class="headerlink" title="统计量图表"></a>统计量图表</h3><table>
<thead>
<tr>
<th>Statistic term统计量</th>
<th>Batch Norm</th>
<th>Layer Norm</th>
<th>Instance Norm</th>
<th>Group Norm</th>
</tr>
</thead>
<tbody><tr>
<td>NLP</td>
<td>[N, L, C] -&gt; [C]</td>
<td>[N, L, C] -&gt; [N, L]</td>
<td>[N, L, C] -&gt; [N, C]</td>
<td>[N, G, L, C/G] -&gt; [N, G]</td>
</tr>
<tr>
<td>CV</td>
<td>[N, C, H, W] -&gt; [C]</td>
<td>[N, C, H, W] -&gt; [N]</td>
<td>[N, C, H, W] -&gt; [N, C]</td>
<td>[N, G, C/G, H, W] -&gt; [N, G]</td>
</tr>
</tbody></table>
<p><strong>最后这里放一段我觉得非常易于理解的比喻</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h2jhplp99pj20r60e5gnk.jpg"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" rel="tag"># 深度学习基础</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/23/%E3%80%8C%E5%B0%8F%E8%84%9A%E6%9C%AC%E3%80%8D%EF%BC%88%E4%B8%80%EF%BC%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E6%A0%87%E7%AD%BE%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="prev" title="「小脚本」（一）目标检测数据标签可视化">
                  <i class="fa fa-chevron-left"></i> 「小脚本」（一）目标检测数据标签可视化
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/23/%E3%80%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8D%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" rel="next" title="「深度学习基础」语义分割损失函数">
                  「深度学习基础」语义分割损失函数 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陨石kk</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>
--!>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>
